{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Text generati\n",
        "on model using LSTM**"
      ],
      "metadata": {
        "id": "t6K7KNsHJAKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Open this link\n",
        " https://www.gutenberg.org/cache/epub/100/pg100.txt\n",
        "\n",
        "Save the file as shakespeare.txt"
      ],
      "metadata": {
        "id": "W-90p8OHJX8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_F0rpQWtHQAq"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text file\n",
        "with open(\"/content/shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation & special characters\n",
        "text = re.sub(r'[^a-z\\s.,;!?]', '', text)\n",
        "\n",
        "\n",
        "print(\"Total characters:\", len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGLg-1pWHyEH",
        "outputId": "61adfcac-b46f-49bb-87f6-e13fc3f28d0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 5319235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Character-Level Tokenization\n",
        "# Create vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Mapping characters to numbers\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# Encode text\n",
        "encoded_text = np.array([char_to_idx[c] for c in text])\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_yJsOZfIFDT",
        "outputId": "9064d14f-212c-4d02-bccd-c56b1d5d1f81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Input–Output Sequences\n",
        "seq_length = 80\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(encoded_text) - seq_length):\n",
        "    X.append(encoded_text[i:i+seq_length])\n",
        "    y.append(encoded_text[i+seq_length])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Input shape:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55-h7nWFIH5t",
        "outputId": "d6deecbf-4ab4-49dd-b632-3560390908b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (5319155, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 100, input_length=seq_length),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam'\n",
        ")\n",
        "\n",
        "# 3️⃣ Train model\n",
        "early_stop = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y6F8ixAIKkC",
        "outputId": "976021b1-075a-4871-dc35-c05f54c74de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m41556/41556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1156s\u001b[0m 28ms/step - loss: 1.6372\n",
            "Epoch 2/5\n",
            "\u001b[1m41556/41556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1149s\u001b[0m 28ms/step - loss: 1.3063\n",
            "Epoch 3/5\n",
            "\u001b[1m41556/41556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1149s\u001b[0m 28ms/step - loss: 1.2733\n",
            "Epoch 4/5\n",
            "\u001b[1m41556/41556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1149s\u001b[0m 28ms/step - loss: 1.2622\n",
            "Epoch 5/5\n",
            "\u001b[1m 7541/41556\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:40\u001b[0m 28ms/step - loss: 1.2506"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"lstm_text_generator.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNJ1_8BhQ-XW",
        "outputId": "5d33520d-8a71-4de6-9b4d-ef243e7b699e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_with_temperature(preds, temperature=0.8):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-8) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(preds), p=preds)\n"
      ],
      "metadata": {
        "id": "gOVzG6GyQs2W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator function\n",
        "def generate_text(seed_text, length=300):\n",
        "    seed_text = seed_text.lower()\n",
        "    generated = seed_text\n",
        "\n",
        "    for _ in range(length):\n",
        "        encoded_seed = [char_to_idx.get(c, 0) for c in seed_text]\n",
        "        encoded_seed = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [encoded_seed], maxlen=seq_length\n",
        "        )\n",
        "\n",
        "        prediction = model.predict(encoded_seed, verbose=0)\n",
        "        next_index = sample_with_temperature(prediction[0], temperature=0.8)\n",
        "        next_char = idx_to_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iAACcL_mIX6n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"to be or not to be\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UFOkS_hQ5M-",
        "outputId": "9c14d18f-fee6-47d5-c092-984c80361dae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to be or not to beliet not a vanish of rands, soothmens the self elspio.\n",
            "\n",
            "reachthum.\n",
            "good masters woodgess.\n",
            "a fellow are pity of greating.\n",
            "\n",
            "gaoler.\n",
            "assounce, gentleman.\n",
            "\n",
            "costard.\n",
            "such a business in the foolock was this bastard.\n",
            "\n",
            "thaisabinbure.\n",
            "\n",
            "paris.\n",
            "gives the earth and servants, cordelia.\n",
            "\n",
            "titus.\n",
            "he shall be sooner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Zucz9_oIYJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}